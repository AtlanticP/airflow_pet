## Airflow pet-project
#### Задача:
Создать ETL-процесс, который получал бы персональные данные лица из источника, обрабатывал их и загружал в DataWarehouse.

1. Имитация API. Генерация персональных данных лица, обращающегося за услугой. Данные хранятся в отдельной папке ***api*** в формате JSON случайным объемом (batch): от 1 до 10 заявок. Данные генерируются с использованием модулей ***random*** и ***fake***. 
	Непосредственно генерация данных ***utils/person.py***. Создание батча ***utils/createbatch.py***.

2. DAG ***create_storage*** создает базу данных dw в PostgreSQL , создает пользователя user1, создает таблицу request, наделяет пользователя правами CRUD на эту таблицу. 

3. DAG ***create_batch*** генерирует каждые 5 секунд батчи - данные, указанные в п.1 

3. DAG ***applications*** осуществляет ETL-процесс:
	- извлекает данные из источника. В моем случае ***api/api.json***,
	- с помощью regexp приводит телефонные номера к последовательности из 10 цифр,
	- с помощью PostgresHook записывает все данные в таблицу ***request***.
	Запланированный интервал выолнения DAGа - 5 секунд.

#### Установка
Для работы необходимо определить переменные среды с паролями, а также домашнюю папку Airflow. Один из возможных вариантов установки:

Клонируем репо и заходим в папку проекта

`cd airflow_pet/`

Создаем виртуальное окружение

`python -m venv .airflow`

Активируем его

`source .airflow/bin/activate`

Устанавливаем необходимые библиотеки

`pip install -r requirements.txt`

Определяем переменную среды AIRFLOW_HOME

`export AIRFLOW_HOME=$(pwd)`

Инициализируем Airflow Backend

`airflow db init`

Создаем пакет

`python setup.py install`

Определяем переменную среды MY_PASSWORD для пользователя user1 в БД. Необходимо для авторизации в СУБД

`export MY_PASSWORD=pass`

Определяем переменную среды SUDO_PASSWORD для суперпользователя. Необходимо для перезагрузки процесса postgresql

`export SUDO_PASSWORD=your_password`
